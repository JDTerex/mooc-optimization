{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton's local method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtonLocal(fct,x0,eps,maxiter=100):\n",
    "    \"\"\"\n",
    "    :param fct: function that returns the value of the function, its gradient and hessian\n",
    "    :type fct: f, g, h = fct(x)\n",
    "    \n",
    "    :param x0: starting point for the algorithm. \n",
    "    :type x0: numpy.array\n",
    "    \n",
    "    :param eps: precision to reach.\n",
    "    :type eps: float.\n",
    "    \n",
    "    :param maxiter: maximum number of iterations. Default: 100.\n",
    "    :type maxiter: int\n",
    "    \n",
    "    :return: x, message, where x is the last value generated by the algorithm, and message the reason why it stopped. \n",
    "    \"\"\"\n",
    "    k = 0\n",
    "    x = x0\n",
    "    f, g, H = fct(x)\n",
    "    while np.linalg.norm(g) > eps and k <= maxiter:\n",
    "        k += 1\n",
    "        try:    \n",
    "            d = np.linalg.solve(H, -g)\n",
    "            x = x + d\n",
    "        except LinAlgError as e:\n",
    "            message = f'Numerical issue encountered in iteration {k}: {e}'\n",
    "            return None, message\n",
    "        f, g, H = fct(x)\n",
    "\n",
    "    if np.linalg.norm(g) <= eps:\n",
    "        return x, f'Required precision has been reached: {np.linalg.norm(g)} <= {eps}'\n",
    "    else:\n",
    "        return None, f'Maximum number of iterations reached: {maxiter}'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the function \\\\[f(x_1,x_2) = 2x_1^3+6x_1x_2^2 -3x_2^3-150x_1. \\\\]\n",
    "\n",
    "Then the gradient is \\\\[ \\nabla f(x) = \\left( \\begin{array}{c} 6x_1^2+6x_2^2-150 \\\\ 12x_1x_2-9x_2^2 \\end{array} \\right), \\\\]\n",
    "and the Hessian is  \\\\[\\nabla^2 f(x) = \\left( \\begin{array}{cc} 12x_1 & 12x_2\\\\12x_2 & 12x_1-18x_2\\end{array} \\right). \\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    f = 2*x[0]**3 + 6*x[0]*x[1]**2 -3*x[1]**3 -150*x[0]\n",
    "    g = np.array([6*x[0]**2 + 6*x[1]**2-150,12*x[0]*x[1]-9*x[1]**2])\n",
    "    h = np.array([[12*x[0], 12*x[1]],[12*x[1], 12*x[0]-18*x[1]]])\n",
    "    return f, g, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply Newton's local algorithm in order to find minima starting from the points $(6.2,-3.4)$ and $(2.8,4.3)$, with $\\varepsilon=10^{-15}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [ 5.00000000e+00 -1.15446625e-22] Diagnostic: Required precision has been reached: 6.926797524821983e-21 <= 1e-15\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([6.2, -3.4])\n",
    "eps = 10**(-15)\n",
    "xstar, message = newtonLocal(func,x0,eps,maxiter=100)\n",
    "print(f'Solution: {xstar} Diagnostic: {message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f = -500.0\n",
      "g = [ 0.00000000e+00 -6.92679752e-21]\n",
      "h = [[ 6.0000000e+01 -1.3853595e-21]\n",
      " [-1.3853595e-21  6.0000000e+01]]\n",
      "Eigenvalues of the Hessian: [60. 60.]\n"
     ]
    }
   ],
   "source": [
    "f, g, h = func(xstar)\n",
    "print(f'f = {f}')\n",
    "print(f'g = {g}')\n",
    "print(f'h = {h}')\n",
    "print(f'Eigenvalues of the Hessian: {np.linalg.eig(h)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is \\\\[ \\nabla f(5,0) = \\left( \\begin{array}{c} 6*5^2+6*0^2-150 \\\\ 12*5*0-9*0^2 \\end{array} \\right) = \\left( \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right), \\\\] and the Hessian\n",
    "\\\\[\\nabla^2 f(5,0) = \\left( \\begin{array}{cc} 12*5 & 12*0\\\\12*0 & 12*5 -18*0\\end{array} \\right) = \\left( \\begin{array}{cc} 60 & 0\\\\0 & 60\\end{array} \\right) \\\\] is positive definite. The sufficient optimality conditions are verified. This is therefore a minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [3. 4.] Diagnostic: Required precision has been reached: 0.0 <= 1e-15\n"
     ]
    }
   ],
   "source": [
    "x1 = np.array([2.8, 4.3])\n",
    "xstar, message = newtonLocal(func,x1,eps,maxiter=100)\n",
    "print(f'Solution: {xstar} Diagnostic: {message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f = -300.0\n",
      "g = [0. 0.]\n",
      "h = [[ 36.  48.]\n",
      " [ 48. -36.]]\n",
      "Eigenvalues of the Hessian: [ 60. -60.]\n"
     ]
    }
   ],
   "source": [
    "f, g, h = func(xstar)\n",
    "print(f'f = {f}')\n",
    "print(f'g = {g}')\n",
    "print(f'h = {h}')\n",
    "print(f'Eigenvalues of the Hessian: {np.linalg.eig(h)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have found a critical point, as the gradient \\\\[ \\nabla f(3,4) = \\left( \\begin{array}{c} 6*3^2+6*4^2-150 \\\\ 12*3*4-9*4^2 \\end{array} \\right) = \\left( \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right). \\\\] \n",
    "The Hessian at the critical point is\n",
    "\\\\[\\nabla^2 f(3,4) = \\left( \\begin{array}{cc} 12*3 & 12*4\\\\12*4 & 12*3 -18*4\\end{array} \\right) = \\left( \\begin{array}{cc} 36 & 48\\\\48 &-36 \\end{array} \\right). \\\\] It is not positive definite. Indeed, the eigenvalues are 60 and -60. Therefore, the necessary optimality conditions are violated, and the critcial point is not a minimum. This is actually a saddle point, as there are both positive and negative eigenvalues. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
